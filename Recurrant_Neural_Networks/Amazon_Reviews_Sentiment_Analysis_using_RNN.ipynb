{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Megha_Komarraju_Lab3_F.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCjjApBrICbx",
        "outputId": "72723454-47fa-4fad-8931-5258916b49b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive,files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kxoe_a4j8Pmj"
      },
      "source": [
        "#Gathering Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKpIRSnmImqC",
        "outputId": "62a1bae5-648b-4c20-d6af-0613b745a71f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pos_path='./drive/My Drive/reviews-Lab-3/reviews/pos/'\n",
        "neg_path='./drive/My Drive//reviews-Lab-3/reviews/neg/'\n",
        "positive_files=os.listdir(pos_path)\n",
        "negative_files=os.listdir(neg_path)\n",
        "max_features=200\n",
        "\n",
        "corpus=[]\n",
        "labels=[]\n",
        "no_reviews=500\n",
        "#Reading 500 documents from positive reviews\n",
        "for i in range(no_reviews):\n",
        "  doc=positive_files[i]\n",
        "  with open(pos_path+doc) as fi:\n",
        "    corpus.append(fi.read().replace('\\n',' '))\n",
        "    labels.append([1,0])\n",
        "#Reading 500 documents from negative reviews\n",
        "for i in range(no_reviews):\n",
        "  doc=negative_files[i]\n",
        "  with open(neg_path+doc) as fi:\n",
        "    corpus.append(fi.read().replace('\\n',' '))\n",
        "    labels.append([0,1])\n",
        "\n",
        "\n",
        "vectorizer=TfidfVectorizer(max_features=max_features,stop_words='english')\n",
        "X=vectorizer.fit_transform(corpus)\n",
        "y=np.array(labels)\n",
        "#print(corpus[:5])\n",
        "print(X.shape,y.shape)\n",
        "print(X[9,14])\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 200) (1000, 2)\n",
            "0.11642945566934004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaKsI5RI8bi2"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyZcOr6ackDF",
        "outputId": "a21100b4-a242-4837-bdbd-1c5d37b87792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "words_length=-1\n",
        "\n",
        "word_tokenizer=vectorizer.build_tokenizer()\n",
        "#print(word_tokenizer)\n",
        "vocab=vectorizer.vocabulary_\n",
        "#One-hot encoding using TF-IDF value instead of 1 and left-padding with zeroes if length is less than maximum number of words present in vocabulary in a document\n",
        "words_list=[word_tokenizer(doc_str) for doc_str in corpus]\n",
        "\n",
        "docs=[]\n",
        "for i in range(len(words_list)):\n",
        "  terms=[]\n",
        "  for j in range(len(words_list[i])):\n",
        "    word=words_list[i][j]\n",
        "    if word in vocab:\n",
        "      terms.append(word)\n",
        "  if len(terms) > words_length:\n",
        "    words_length=len(terms)\n",
        "  docs.append(terms)\n",
        "\n",
        "datasets=np.zeros((X.shape[0],words_length,max_features))\n",
        "print(datasets.shape)\n",
        "print(len(docs))\n",
        "print(len(terms))\n",
        "print(words_length)\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  #print(len(docs[i]))\n",
        "  no_padding=words_length-len(docs[i])\n",
        "  #print(no_padding)\n",
        "\n",
        "  for j in range(len(docs[i])):\n",
        "    w=docs[i][j]\n",
        "    idx=vocab[w]\n",
        "    tfidf_val=X[i,idx]\n",
        "    datasets[i,j+no_padding,idx]=tfidf_val\n",
        "\n",
        "datasets=datasets.astype(np.float32)\n",
        "y=y.astype(np.float32)\n",
        "#Creating training and testing data\n",
        "X_train,X_val,y_train,y_val=train_test_split(datasets,y,test_size=0.2,random_state=2020)\n",
        "print(X_train.shape,y_train.shape,X_val.shape,y_val.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 310, 200)\n",
            "1000\n",
            "78\n",
            "310\n",
            "(800, 310, 200) (800, 2) (200, 310, 200) (200, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_XHugp58e12"
      },
      "source": [
        "#Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB-qRq90aIZh"
      },
      "source": [
        "from torch.utils.data import DataLoader,TensorDataset\n",
        "\n",
        "batch_size=16\n",
        "\n",
        "training_data=TensorDataset(torch.from_numpy(X_train),torch.from_numpy(y_train))\n",
        "validation_data=TensorDataset(torch.from_numpy(X_val),torch.from_numpy(y_val))\n",
        "\n",
        "train_loader=DataLoader(training_data,shuffle=True,batch_size=batch_size)\n",
        "val_loader=DataLoader(validation_data,shuffle=True,batch_size=batch_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-7jxYvS8jZ1"
      },
      "source": [
        "#Running the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZPaDTye4j2A",
        "outputId": "01866655-9a68-4573-9300-eda92d708d9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.n_layers = n_layers\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,n_layers,batch_first=True) # rnn layer\n",
        "    self.fc1 = nn.Linear(hidden_size,output_size) # rnn output (y_t) --> output (y'_t)\n",
        "    self.fc2 = nn.Linear(output_size,2) #the output from the last time period ->sentiment prediction\n",
        "  def forward(self,x, hidden):\n",
        "    batch_size = x.size()[0]\n",
        "    hidden = self.init_hidden(batch_size)\n",
        "    \n",
        "    rnn_out,hidden = self.rnn(x,hidden)\n",
        "    rnn_out = self.fc1(rnn_out)\n",
        "    last_out = rnn_out[:,-1,:].view(batch_size,-1)\n",
        "    out = F.softmax(self.fc2(last_out))\n",
        "    return out,hidden \n",
        "  def init_hidden(self,batch_size):\n",
        "    hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size).cuda()\n",
        "    return hidden\n",
        "model = Model(200,32,256,3)\n",
        "print(model)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(\n",
            "  (rnn): RNN(200, 256, num_layers=3, batch_first=True)\n",
            "  (fc1): Linear(in_features=256, out_features=32, bias=True)\n",
            "  (fc2): Linear(in_features=32, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWjQvv6E8mEg"
      },
      "source": [
        "#Training and Validating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq9Ds4Ug4ymF",
        "outputId": "eeaada23-99f7-4e56-b6d8-37141e213ccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "\n",
        "n_epochs=10\n",
        "lr=1e-5\n",
        "b=0\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
        "#Model Training\n",
        "model.train()\n",
        "for epoch in range(n_epochs):\n",
        "  hidden=model.init_hidden(batch_size)\n",
        "\n",
        "  for data,labels in train_loader:\n",
        "    b+=1\n",
        "    data,labels=data.to(device),labels.to(device)\n",
        "\n",
        "    model.zero_grad()\n",
        "    output,hidden=model(data,hidden)\n",
        "    pred_output=torch.max(labels,1)[1]\n",
        "    loss=criterion(output,pred_output)\n",
        "    loss.backward()\n",
        "   \n",
        "    optimizer.step()\n",
        "\n",
        "    val_hidden=model.init_hidden(batch_size).cuda()\n",
        "    val_losses=[]\n",
        "#Model Validation\n",
        "    model.eval()\n",
        "\n",
        "    for data,labels in val_loader:\n",
        "        data,labels=data.to(device),labels.to(device)\n",
        "        val_outputs,val_hidden=model(data,val_hidden)\n",
        "        pred_output=torch.max(labels,1)[1]\n",
        "        val_loss=criterion(val_outputs,pred_output)\n",
        "        val_losses.append(val_loss.item())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    print('Epoch:{}/{}'.format(epoch,n_epochs), # epoch is the index of epoch\n",
        "    'Batch:{}'.format(b),  # b is the index of batch\n",
        "    'Train Loss:{:.5f}'.format(loss.item()),\n",
        "    'Val Loss:{:.5f}'.format(np.mean(val_losses)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:0/10 Batch:1 Train Loss:0.69249 Val Loss:0.69346\n",
            "Epoch:0/10 Batch:2 Train Loss:0.69641 Val Loss:0.69343\n",
            "Epoch:0/10 Batch:3 Train Loss:0.69126 Val Loss:0.69335\n",
            "Epoch:0/10 Batch:4 Train Loss:0.69340 Val Loss:0.69357\n",
            "Epoch:0/10 Batch:5 Train Loss:0.69600 Val Loss:0.69334\n",
            "Epoch:0/10 Batch:6 Train Loss:0.69220 Val Loss:0.69334\n",
            "Epoch:0/10 Batch:7 Train Loss:0.69418 Val Loss:0.69333\n",
            "Epoch:0/10 Batch:8 Train Loss:0.69338 Val Loss:0.69326\n",
            "Epoch:0/10 Batch:9 Train Loss:0.69337 Val Loss:0.69342\n",
            "Epoch:0/10 Batch:10 Train Loss:0.69250 Val Loss:0.69326\n",
            "Epoch:0/10 Batch:11 Train Loss:0.69390 Val Loss:0.69340\n",
            "Epoch:0/10 Batch:12 Train Loss:0.69251 Val Loss:0.69329\n",
            "Epoch:0/10 Batch:13 Train Loss:0.69182 Val Loss:0.69330\n",
            "Epoch:0/10 Batch:14 Train Loss:0.69384 Val Loss:0.69329\n",
            "Epoch:0/10 Batch:15 Train Loss:0.69232 Val Loss:0.69329\n",
            "Epoch:0/10 Batch:16 Train Loss:0.69233 Val Loss:0.69337\n",
            "Epoch:0/10 Batch:17 Train Loss:0.69319 Val Loss:0.69337\n",
            "Epoch:0/10 Batch:18 Train Loss:0.69323 Val Loss:0.69349\n",
            "Epoch:0/10 Batch:19 Train Loss:0.69327 Val Loss:0.69338\n",
            "Epoch:0/10 Batch:20 Train Loss:0.69028 Val Loss:0.69340\n",
            "Epoch:0/10 Batch:21 Train Loss:0.69243 Val Loss:0.69335\n",
            "Epoch:0/10 Batch:22 Train Loss:0.69221 Val Loss:0.69337\n",
            "Epoch:0/10 Batch:23 Train Loss:0.69443 Val Loss:0.69345\n",
            "Epoch:0/10 Batch:24 Train Loss:0.69114 Val Loss:0.69354\n",
            "Epoch:0/10 Batch:25 Train Loss:0.69117 Val Loss:0.69333\n",
            "Epoch:0/10 Batch:26 Train Loss:0.69294 Val Loss:0.69342\n",
            "Epoch:0/10 Batch:27 Train Loss:0.69561 Val Loss:0.69342\n",
            "Epoch:0/10 Batch:28 Train Loss:0.70108 Val Loss:0.69352\n",
            "Epoch:0/10 Batch:29 Train Loss:0.69053 Val Loss:0.69341\n",
            "Epoch:0/10 Batch:30 Train Loss:0.69194 Val Loss:0.69363\n",
            "Epoch:0/10 Batch:31 Train Loss:0.69081 Val Loss:0.69343\n",
            "Epoch:0/10 Batch:32 Train Loss:0.69186 Val Loss:0.69322\n",
            "Epoch:0/10 Batch:33 Train Loss:0.69210 Val Loss:0.69334\n",
            "Epoch:0/10 Batch:34 Train Loss:0.69025 Val Loss:0.69345\n",
            "Epoch:0/10 Batch:35 Train Loss:0.69603 Val Loss:0.69336\n",
            "Epoch:0/10 Batch:36 Train Loss:0.69338 Val Loss:0.69347\n",
            "Epoch:0/10 Batch:37 Train Loss:0.69170 Val Loss:0.69325\n",
            "Epoch:0/10 Batch:38 Train Loss:0.69320 Val Loss:0.69359\n",
            "Epoch:0/10 Batch:39 Train Loss:0.69336 Val Loss:0.69361\n",
            "Epoch:0/10 Batch:40 Train Loss:0.69474 Val Loss:0.69372\n",
            "Epoch:0/10 Batch:41 Train Loss:0.69494 Val Loss:0.69347\n",
            "Epoch:0/10 Batch:42 Train Loss:0.70060 Val Loss:0.69358\n",
            "Epoch:0/10 Batch:43 Train Loss:0.69459 Val Loss:0.69356\n",
            "Epoch:0/10 Batch:44 Train Loss:0.69191 Val Loss:0.69345\n",
            "Epoch:0/10 Batch:45 Train Loss:0.69326 Val Loss:0.69354\n",
            "Epoch:0/10 Batch:46 Train Loss:0.69320 Val Loss:0.69333\n",
            "Epoch:0/10 Batch:47 Train Loss:0.69181 Val Loss:0.69323\n",
            "Epoch:0/10 Batch:48 Train Loss:0.69298 Val Loss:0.69334\n",
            "Epoch:0/10 Batch:49 Train Loss:0.69444 Val Loss:0.69331\n",
            "Epoch:0/10 Batch:50 Train Loss:0.69457 Val Loss:0.69350\n",
            "Epoch:1/10 Batch:51 Train Loss:0.69672 Val Loss:0.69321\n",
            "Epoch:1/10 Batch:52 Train Loss:0.69309 Val Loss:0.69330\n",
            "Epoch:1/10 Batch:53 Train Loss:0.69208 Val Loss:0.69320\n",
            "Epoch:1/10 Batch:54 Train Loss:0.69212 Val Loss:0.69359\n",
            "Epoch:1/10 Batch:55 Train Loss:0.69228 Val Loss:0.69342\n",
            "Epoch:1/10 Batch:56 Train Loss:0.69304 Val Loss:0.69357\n",
            "Epoch:1/10 Batch:57 Train Loss:0.69296 Val Loss:0.69365\n",
            "Epoch:1/10 Batch:58 Train Loss:0.69336 Val Loss:0.69334\n",
            "Epoch:1/10 Batch:59 Train Loss:0.69412 Val Loss:0.69321\n",
            "Epoch:1/10 Batch:60 Train Loss:0.69210 Val Loss:0.69327\n",
            "Epoch:1/10 Batch:61 Train Loss:0.69312 Val Loss:0.69347\n",
            "Epoch:1/10 Batch:62 Train Loss:0.69520 Val Loss:0.69340\n",
            "Epoch:1/10 Batch:63 Train Loss:0.69500 Val Loss:0.69325\n",
            "Epoch:1/10 Batch:64 Train Loss:0.69331 Val Loss:0.69344\n",
            "Epoch:1/10 Batch:65 Train Loss:0.69173 Val Loss:0.69319\n",
            "Epoch:1/10 Batch:66 Train Loss:0.69102 Val Loss:0.69324\n",
            "Epoch:1/10 Batch:67 Train Loss:0.69324 Val Loss:0.69343\n",
            "Epoch:1/10 Batch:68 Train Loss:0.69315 Val Loss:0.69325\n",
            "Epoch:1/10 Batch:69 Train Loss:0.69457 Val Loss:0.69344\n",
            "Epoch:1/10 Batch:70 Train Loss:0.69160 Val Loss:0.69348\n",
            "Epoch:1/10 Batch:71 Train Loss:0.69321 Val Loss:0.69326\n",
            "Epoch:1/10 Batch:72 Train Loss:0.69475 Val Loss:0.69325\n",
            "Epoch:1/10 Batch:73 Train Loss:0.69391 Val Loss:0.69320\n",
            "Epoch:1/10 Batch:74 Train Loss:0.69441 Val Loss:0.69319\n",
            "Epoch:1/10 Batch:75 Train Loss:0.69048 Val Loss:0.69314\n",
            "Epoch:1/10 Batch:76 Train Loss:0.69233 Val Loss:0.69336\n",
            "Epoch:1/10 Batch:77 Train Loss:0.69236 Val Loss:0.69336\n",
            "Epoch:1/10 Batch:78 Train Loss:0.69319 Val Loss:0.69314\n",
            "Epoch:1/10 Batch:79 Train Loss:0.69390 Val Loss:0.69335\n",
            "Epoch:1/10 Batch:80 Train Loss:0.69018 Val Loss:0.69338\n",
            "Epoch:1/10 Batch:81 Train Loss:0.69317 Val Loss:0.69318\n",
            "Epoch:1/10 Batch:82 Train Loss:0.69233 Val Loss:0.69320\n",
            "Epoch:1/10 Batch:83 Train Loss:0.69491 Val Loss:0.69333\n",
            "Epoch:1/10 Batch:84 Train Loss:0.69285 Val Loss:0.69341\n",
            "Epoch:1/10 Batch:85 Train Loss:0.69683 Val Loss:0.69320\n",
            "Epoch:1/10 Batch:86 Train Loss:0.69142 Val Loss:0.69327\n",
            "Epoch:1/10 Batch:87 Train Loss:0.69688 Val Loss:0.69332\n",
            "Epoch:1/10 Batch:88 Train Loss:0.69487 Val Loss:0.69344\n",
            "Epoch:1/10 Batch:89 Train Loss:0.69305 Val Loss:0.69318\n",
            "Epoch:1/10 Batch:90 Train Loss:0.69403 Val Loss:0.69335\n",
            "Epoch:1/10 Batch:91 Train Loss:0.69057 Val Loss:0.69329\n",
            "Epoch:1/10 Batch:92 Train Loss:0.69443 Val Loss:0.69325\n",
            "Epoch:1/10 Batch:93 Train Loss:0.69252 Val Loss:0.69323\n",
            "Epoch:1/10 Batch:94 Train Loss:0.69444 Val Loss:0.69318\n",
            "Epoch:1/10 Batch:95 Train Loss:0.69212 Val Loss:0.69327\n",
            "Epoch:1/10 Batch:96 Train Loss:0.69168 Val Loss:0.69327\n",
            "Epoch:1/10 Batch:97 Train Loss:0.69361 Val Loss:0.69332\n",
            "Epoch:1/10 Batch:98 Train Loss:0.69332 Val Loss:0.69320\n",
            "Epoch:1/10 Batch:99 Train Loss:0.69250 Val Loss:0.69333\n",
            "Epoch:1/10 Batch:100 Train Loss:0.69246 Val Loss:0.69333\n",
            "Epoch:2/10 Batch:101 Train Loss:0.69641 Val Loss:0.69313\n",
            "Epoch:2/10 Batch:102 Train Loss:0.69239 Val Loss:0.69337\n",
            "Epoch:2/10 Batch:103 Train Loss:0.69381 Val Loss:0.69327\n",
            "Epoch:2/10 Batch:104 Train Loss:0.69100 Val Loss:0.69335\n",
            "Epoch:2/10 Batch:105 Train Loss:0.69375 Val Loss:0.69328\n",
            "Epoch:2/10 Batch:106 Train Loss:0.69297 Val Loss:0.69328\n",
            "Epoch:2/10 Batch:107 Train Loss:0.69395 Val Loss:0.69331\n",
            "Epoch:2/10 Batch:108 Train Loss:0.69205 Val Loss:0.69328\n",
            "Epoch:2/10 Batch:109 Train Loss:0.69368 Val Loss:0.69333\n",
            "Epoch:2/10 Batch:110 Train Loss:0.69533 Val Loss:0.69322\n",
            "Epoch:2/10 Batch:111 Train Loss:0.69310 Val Loss:0.69317\n",
            "Epoch:2/10 Batch:112 Train Loss:0.69395 Val Loss:0.69322\n",
            "Epoch:2/10 Batch:113 Train Loss:0.69445 Val Loss:0.69324\n",
            "Epoch:2/10 Batch:114 Train Loss:0.69409 Val Loss:0.69323\n",
            "Epoch:2/10 Batch:115 Train Loss:0.69314 Val Loss:0.69322\n",
            "Epoch:2/10 Batch:116 Train Loss:0.69372 Val Loss:0.69320\n",
            "Epoch:2/10 Batch:117 Train Loss:0.69275 Val Loss:0.69321\n",
            "Epoch:2/10 Batch:118 Train Loss:0.69281 Val Loss:0.69318\n",
            "Epoch:2/10 Batch:119 Train Loss:0.69292 Val Loss:0.69319\n",
            "Epoch:2/10 Batch:120 Train Loss:0.69295 Val Loss:0.69318\n",
            "Epoch:2/10 Batch:121 Train Loss:0.69294 Val Loss:0.69318\n",
            "Epoch:2/10 Batch:122 Train Loss:0.69323 Val Loss:0.69319\n",
            "Epoch:2/10 Batch:123 Train Loss:0.69314 Val Loss:0.69318\n",
            "Epoch:2/10 Batch:124 Train Loss:0.69281 Val Loss:0.69319\n",
            "Epoch:2/10 Batch:125 Train Loss:0.69279 Val Loss:0.69322\n",
            "Epoch:2/10 Batch:126 Train Loss:0.69341 Val Loss:0.69326\n",
            "Epoch:2/10 Batch:127 Train Loss:0.69352 Val Loss:0.69318\n",
            "Epoch:2/10 Batch:128 Train Loss:0.69253 Val Loss:0.69324\n",
            "Epoch:2/10 Batch:129 Train Loss:0.69356 Val Loss:0.69323\n",
            "Epoch:2/10 Batch:130 Train Loss:0.69298 Val Loss:0.69323\n",
            "Epoch:2/10 Batch:131 Train Loss:0.69302 Val Loss:0.69333\n",
            "Epoch:2/10 Batch:132 Train Loss:0.69222 Val Loss:0.69317\n",
            "Epoch:2/10 Batch:133 Train Loss:0.69324 Val Loss:0.69319\n",
            "Epoch:2/10 Batch:134 Train Loss:0.69323 Val Loss:0.69333\n",
            "Epoch:2/10 Batch:135 Train Loss:0.69387 Val Loss:0.69323\n",
            "Epoch:2/10 Batch:136 Train Loss:0.69307 Val Loss:0.69321\n",
            "Epoch:2/10 Batch:137 Train Loss:0.69374 Val Loss:0.69330\n",
            "Epoch:2/10 Batch:138 Train Loss:0.69465 Val Loss:0.69313\n",
            "Epoch:2/10 Batch:139 Train Loss:0.69362 Val Loss:0.69325\n",
            "Epoch:2/10 Batch:140 Train Loss:0.69272 Val Loss:0.69324\n",
            "Epoch:2/10 Batch:141 Train Loss:0.69269 Val Loss:0.69330\n",
            "Epoch:2/10 Batch:142 Train Loss:0.69209 Val Loss:0.69322\n",
            "Epoch:2/10 Batch:143 Train Loss:0.69257 Val Loss:0.69326\n",
            "Epoch:2/10 Batch:144 Train Loss:0.69367 Val Loss:0.69324\n",
            "Epoch:2/10 Batch:145 Train Loss:0.69401 Val Loss:0.69325\n",
            "Epoch:2/10 Batch:146 Train Loss:0.69223 Val Loss:0.69321\n",
            "Epoch:2/10 Batch:147 Train Loss:0.69261 Val Loss:0.69330\n",
            "Epoch:2/10 Batch:148 Train Loss:0.69278 Val Loss:0.69317\n",
            "Epoch:2/10 Batch:149 Train Loss:0.69153 Val Loss:0.69327\n",
            "Epoch:2/10 Batch:150 Train Loss:0.69381 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:151 Train Loss:0.69412 Val Loss:0.69338\n",
            "Epoch:3/10 Batch:152 Train Loss:0.69155 Val Loss:0.69333\n",
            "Epoch:3/10 Batch:153 Train Loss:0.69109 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:154 Train Loss:0.69369 Val Loss:0.69343\n",
            "Epoch:3/10 Batch:155 Train Loss:0.69305 Val Loss:0.69325\n",
            "Epoch:3/10 Batch:156 Train Loss:0.69300 Val Loss:0.69326\n",
            "Epoch:3/10 Batch:157 Train Loss:0.69207 Val Loss:0.69353\n",
            "Epoch:3/10 Batch:158 Train Loss:0.69562 Val Loss:0.69332\n",
            "Epoch:3/10 Batch:159 Train Loss:0.69493 Val Loss:0.69347\n",
            "Epoch:3/10 Batch:160 Train Loss:0.69634 Val Loss:0.69337\n",
            "Epoch:3/10 Batch:161 Train Loss:0.69401 Val Loss:0.69313\n",
            "Epoch:3/10 Batch:162 Train Loss:0.69689 Val Loss:0.69318\n",
            "Epoch:3/10 Batch:163 Train Loss:0.69113 Val Loss:0.69318\n",
            "Epoch:3/10 Batch:164 Train Loss:0.69126 Val Loss:0.69332\n",
            "Epoch:3/10 Batch:165 Train Loss:0.69311 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:166 Train Loss:0.69365 Val Loss:0.69322\n",
            "Epoch:3/10 Batch:167 Train Loss:0.69298 Val Loss:0.69327\n",
            "Epoch:3/10 Batch:168 Train Loss:0.69150 Val Loss:0.69323\n",
            "Epoch:3/10 Batch:169 Train Loss:0.69305 Val Loss:0.69322\n",
            "Epoch:3/10 Batch:170 Train Loss:0.69478 Val Loss:0.69325\n",
            "Epoch:3/10 Batch:171 Train Loss:0.69428 Val Loss:0.69322\n",
            "Epoch:3/10 Batch:172 Train Loss:0.69325 Val Loss:0.69321\n",
            "Epoch:3/10 Batch:173 Train Loss:0.69337 Val Loss:0.69326\n",
            "Epoch:3/10 Batch:174 Train Loss:0.69281 Val Loss:0.69324\n",
            "Epoch:3/10 Batch:175 Train Loss:0.69200 Val Loss:0.69317\n",
            "Epoch:3/10 Batch:176 Train Loss:0.69329 Val Loss:0.69325\n",
            "Epoch:3/10 Batch:177 Train Loss:0.69260 Val Loss:0.69323\n",
            "Epoch:3/10 Batch:178 Train Loss:0.69358 Val Loss:0.69327\n",
            "Epoch:3/10 Batch:179 Train Loss:0.69383 Val Loss:0.69326\n",
            "Epoch:3/10 Batch:180 Train Loss:0.69173 Val Loss:0.69330\n",
            "Epoch:3/10 Batch:181 Train Loss:0.69223 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:182 Train Loss:0.69479 Val Loss:0.69324\n",
            "Epoch:3/10 Batch:183 Train Loss:0.69311 Val Loss:0.69320\n",
            "Epoch:3/10 Batch:184 Train Loss:0.69386 Val Loss:0.69322\n",
            "Epoch:3/10 Batch:185 Train Loss:0.69268 Val Loss:0.69323\n",
            "Epoch:3/10 Batch:186 Train Loss:0.69239 Val Loss:0.69326\n",
            "Epoch:3/10 Batch:187 Train Loss:0.69303 Val Loss:0.69320\n",
            "Epoch:3/10 Batch:188 Train Loss:0.69236 Val Loss:0.69318\n",
            "Epoch:3/10 Batch:189 Train Loss:0.69148 Val Loss:0.69332\n",
            "Epoch:3/10 Batch:190 Train Loss:0.69411 Val Loss:0.69329\n",
            "Epoch:3/10 Batch:191 Train Loss:0.69256 Val Loss:0.69334\n",
            "Epoch:3/10 Batch:192 Train Loss:0.69316 Val Loss:0.69327\n",
            "Epoch:3/10 Batch:193 Train Loss:0.69253 Val Loss:0.69332\n",
            "Epoch:3/10 Batch:194 Train Loss:0.69242 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:195 Train Loss:0.69234 Val Loss:0.69328\n",
            "Epoch:3/10 Batch:196 Train Loss:0.69182 Val Loss:0.69336\n",
            "Epoch:3/10 Batch:197 Train Loss:0.69380 Val Loss:0.69325\n",
            "Epoch:3/10 Batch:198 Train Loss:0.69420 Val Loss:0.69331\n",
            "Epoch:3/10 Batch:199 Train Loss:0.69336 Val Loss:0.69314\n",
            "Epoch:3/10 Batch:200 Train Loss:0.69476 Val Loss:0.69344\n",
            "Epoch:4/10 Batch:201 Train Loss:0.69336 Val Loss:0.69334\n",
            "Epoch:4/10 Batch:202 Train Loss:0.69384 Val Loss:0.69333\n",
            "Epoch:4/10 Batch:203 Train Loss:0.69309 Val Loss:0.69332\n",
            "Epoch:4/10 Batch:204 Train Loss:0.69324 Val Loss:0.69332\n",
            "Epoch:4/10 Batch:205 Train Loss:0.69477 Val Loss:0.69325\n",
            "Epoch:4/10 Batch:206 Train Loss:0.69236 Val Loss:0.69331\n",
            "Epoch:4/10 Batch:207 Train Loss:0.69552 Val Loss:0.69324\n",
            "Epoch:4/10 Batch:208 Train Loss:0.69083 Val Loss:0.69325\n",
            "Epoch:4/10 Batch:209 Train Loss:0.69248 Val Loss:0.69319\n",
            "Epoch:4/10 Batch:210 Train Loss:0.69297 Val Loss:0.69325\n",
            "Epoch:4/10 Batch:211 Train Loss:0.69540 Val Loss:0.69320\n",
            "Epoch:4/10 Batch:212 Train Loss:0.69171 Val Loss:0.69312\n",
            "Epoch:4/10 Batch:213 Train Loss:0.69523 Val Loss:0.69335\n",
            "Epoch:4/10 Batch:214 Train Loss:0.69391 Val Loss:0.69329\n",
            "Epoch:4/10 Batch:215 Train Loss:0.69117 Val Loss:0.69344\n",
            "Epoch:4/10 Batch:216 Train Loss:0.69247 Val Loss:0.69322\n",
            "Epoch:4/10 Batch:217 Train Loss:0.69239 Val Loss:0.69338\n",
            "Epoch:4/10 Batch:218 Train Loss:0.69364 Val Loss:0.69333\n",
            "Epoch:4/10 Batch:219 Train Loss:0.69302 Val Loss:0.69317\n",
            "Epoch:4/10 Batch:220 Train Loss:0.69092 Val Loss:0.69334\n",
            "Epoch:4/10 Batch:221 Train Loss:0.69596 Val Loss:0.69329\n",
            "Epoch:4/10 Batch:222 Train Loss:0.69213 Val Loss:0.69324\n",
            "Epoch:4/10 Batch:223 Train Loss:0.69438 Val Loss:0.69334\n",
            "Epoch:4/10 Batch:224 Train Loss:0.69509 Val Loss:0.69328\n",
            "Epoch:4/10 Batch:225 Train Loss:0.69366 Val Loss:0.69331\n",
            "Epoch:4/10 Batch:226 Train Loss:0.69420 Val Loss:0.69325\n",
            "Epoch:4/10 Batch:227 Train Loss:0.69286 Val Loss:0.69321\n",
            "Epoch:4/10 Batch:228 Train Loss:0.69182 Val Loss:0.69331\n",
            "Epoch:4/10 Batch:229 Train Loss:0.69351 Val Loss:0.69327\n",
            "Epoch:4/10 Batch:230 Train Loss:0.69166 Val Loss:0.69327\n",
            "Epoch:4/10 Batch:231 Train Loss:0.69300 Val Loss:0.69321\n",
            "Epoch:4/10 Batch:232 Train Loss:0.69416 Val Loss:0.69316\n",
            "Epoch:4/10 Batch:233 Train Loss:0.69218 Val Loss:0.69321\n",
            "Epoch:4/10 Batch:234 Train Loss:0.69323 Val Loss:0.69317\n",
            "Epoch:4/10 Batch:235 Train Loss:0.69367 Val Loss:0.69319\n",
            "Epoch:4/10 Batch:236 Train Loss:0.69288 Val Loss:0.69318\n",
            "Epoch:4/10 Batch:237 Train Loss:0.69207 Val Loss:0.69326\n",
            "Epoch:4/10 Batch:238 Train Loss:0.69366 Val Loss:0.69329\n",
            "Epoch:4/10 Batch:239 Train Loss:0.69226 Val Loss:0.69327\n",
            "Epoch:4/10 Batch:240 Train Loss:0.69493 Val Loss:0.69330\n",
            "Epoch:4/10 Batch:241 Train Loss:0.69259 Val Loss:0.69323\n",
            "Epoch:4/10 Batch:242 Train Loss:0.69236 Val Loss:0.69326\n",
            "Epoch:4/10 Batch:243 Train Loss:0.69258 Val Loss:0.69320\n",
            "Epoch:4/10 Batch:244 Train Loss:0.69347 Val Loss:0.69324\n",
            "Epoch:4/10 Batch:245 Train Loss:0.69314 Val Loss:0.69326\n",
            "Epoch:4/10 Batch:246 Train Loss:0.69416 Val Loss:0.69328\n",
            "Epoch:4/10 Batch:247 Train Loss:0.69248 Val Loss:0.69317\n",
            "Epoch:4/10 Batch:248 Train Loss:0.69169 Val Loss:0.69322\n",
            "Epoch:4/10 Batch:249 Train Loss:0.69179 Val Loss:0.69331\n",
            "Epoch:4/10 Batch:250 Train Loss:0.69305 Val Loss:0.69332\n",
            "Epoch:5/10 Batch:251 Train Loss:0.69208 Val Loss:0.69321\n",
            "Epoch:5/10 Batch:252 Train Loss:0.69249 Val Loss:0.69323\n",
            "Epoch:5/10 Batch:253 Train Loss:0.69364 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:254 Train Loss:0.69238 Val Loss:0.69334\n",
            "Epoch:5/10 Batch:255 Train Loss:0.69549 Val Loss:0.69329\n",
            "Epoch:5/10 Batch:256 Train Loss:0.69318 Val Loss:0.69318\n",
            "Epoch:5/10 Batch:257 Train Loss:0.69261 Val Loss:0.69335\n",
            "Epoch:5/10 Batch:258 Train Loss:0.69464 Val Loss:0.69330\n",
            "Epoch:5/10 Batch:259 Train Loss:0.69373 Val Loss:0.69330\n",
            "Epoch:5/10 Batch:260 Train Loss:0.69439 Val Loss:0.69329\n",
            "Epoch:5/10 Batch:261 Train Loss:0.69640 Val Loss:0.69329\n",
            "Epoch:5/10 Batch:262 Train Loss:0.69158 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:263 Train Loss:0.69416 Val Loss:0.69317\n",
            "Epoch:5/10 Batch:264 Train Loss:0.69314 Val Loss:0.69327\n",
            "Epoch:5/10 Batch:265 Train Loss:0.69301 Val Loss:0.69331\n",
            "Epoch:5/10 Batch:266 Train Loss:0.69101 Val Loss:0.69330\n",
            "Epoch:5/10 Batch:267 Train Loss:0.69230 Val Loss:0.69331\n",
            "Epoch:5/10 Batch:268 Train Loss:0.69302 Val Loss:0.69318\n",
            "Epoch:5/10 Batch:269 Train Loss:0.69237 Val Loss:0.69326\n",
            "Epoch:5/10 Batch:270 Train Loss:0.69378 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:271 Train Loss:0.69196 Val Loss:0.69327\n",
            "Epoch:5/10 Batch:272 Train Loss:0.69376 Val Loss:0.69336\n",
            "Epoch:5/10 Batch:273 Train Loss:0.69289 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:274 Train Loss:0.69310 Val Loss:0.69327\n",
            "Epoch:5/10 Batch:275 Train Loss:0.69300 Val Loss:0.69322\n",
            "Epoch:5/10 Batch:276 Train Loss:0.69299 Val Loss:0.69324\n",
            "Epoch:5/10 Batch:277 Train Loss:0.69439 Val Loss:0.69332\n",
            "Epoch:5/10 Batch:278 Train Loss:0.69243 Val Loss:0.69335\n",
            "Epoch:5/10 Batch:279 Train Loss:0.69230 Val Loss:0.69318\n",
            "Epoch:5/10 Batch:280 Train Loss:0.69311 Val Loss:0.69313\n",
            "Epoch:5/10 Batch:281 Train Loss:0.69357 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:282 Train Loss:0.69365 Val Loss:0.69331\n",
            "Epoch:5/10 Batch:283 Train Loss:0.69312 Val Loss:0.69326\n",
            "Epoch:5/10 Batch:284 Train Loss:0.69267 Val Loss:0.69327\n",
            "Epoch:5/10 Batch:285 Train Loss:0.69364 Val Loss:0.69326\n",
            "Epoch:5/10 Batch:286 Train Loss:0.69120 Val Loss:0.69319\n",
            "Epoch:5/10 Batch:287 Train Loss:0.69173 Val Loss:0.69327\n",
            "Epoch:5/10 Batch:288 Train Loss:0.69239 Val Loss:0.69329\n",
            "Epoch:5/10 Batch:289 Train Loss:0.69316 Val Loss:0.69328\n",
            "Epoch:5/10 Batch:290 Train Loss:0.69187 Val Loss:0.69341\n",
            "Epoch:5/10 Batch:291 Train Loss:0.69655 Val Loss:0.69340\n",
            "Epoch:5/10 Batch:292 Train Loss:0.69371 Val Loss:0.69336\n",
            "Epoch:5/10 Batch:293 Train Loss:0.69289 Val Loss:0.69324\n",
            "Epoch:5/10 Batch:294 Train Loss:0.69022 Val Loss:0.69324\n",
            "Epoch:5/10 Batch:295 Train Loss:0.69258 Val Loss:0.69348\n",
            "Epoch:5/10 Batch:296 Train Loss:0.69248 Val Loss:0.69338\n",
            "Epoch:5/10 Batch:297 Train Loss:0.69300 Val Loss:0.69330\n",
            "Epoch:5/10 Batch:298 Train Loss:0.69537 Val Loss:0.69331\n",
            "Epoch:5/10 Batch:299 Train Loss:0.69215 Val Loss:0.69320\n",
            "Epoch:5/10 Batch:300 Train Loss:0.69291 Val Loss:0.69338\n",
            "Epoch:6/10 Batch:301 Train Loss:0.69393 Val Loss:0.69325\n",
            "Epoch:6/10 Batch:302 Train Loss:0.69562 Val Loss:0.69332\n",
            "Epoch:6/10 Batch:303 Train Loss:0.69221 Val Loss:0.69338\n",
            "Epoch:6/10 Batch:304 Train Loss:0.69379 Val Loss:0.69330\n",
            "Epoch:6/10 Batch:305 Train Loss:0.69377 Val Loss:0.69318\n",
            "Epoch:6/10 Batch:306 Train Loss:0.69520 Val Loss:0.69329\n",
            "Epoch:6/10 Batch:307 Train Loss:0.69097 Val Loss:0.69323\n",
            "Epoch:6/10 Batch:308 Train Loss:0.69298 Val Loss:0.69333\n",
            "Epoch:6/10 Batch:309 Train Loss:0.69289 Val Loss:0.69333\n",
            "Epoch:6/10 Batch:310 Train Loss:0.69232 Val Loss:0.69333\n",
            "Epoch:6/10 Batch:311 Train Loss:0.69380 Val Loss:0.69337\n",
            "Epoch:6/10 Batch:312 Train Loss:0.69380 Val Loss:0.69328\n",
            "Epoch:6/10 Batch:313 Train Loss:0.69323 Val Loss:0.69322\n",
            "Epoch:6/10 Batch:314 Train Loss:0.69252 Val Loss:0.69323\n",
            "Epoch:6/10 Batch:315 Train Loss:0.69300 Val Loss:0.69325\n",
            "Epoch:6/10 Batch:316 Train Loss:0.69265 Val Loss:0.69333\n",
            "Epoch:6/10 Batch:317 Train Loss:0.69176 Val Loss:0.69327\n",
            "Epoch:6/10 Batch:318 Train Loss:0.69374 Val Loss:0.69322\n",
            "Epoch:6/10 Batch:319 Train Loss:0.69294 Val Loss:0.69324\n",
            "Epoch:6/10 Batch:320 Train Loss:0.69412 Val Loss:0.69327\n",
            "Epoch:6/10 Batch:321 Train Loss:0.69445 Val Loss:0.69323\n",
            "Epoch:6/10 Batch:322 Train Loss:0.69324 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:323 Train Loss:0.69330 Val Loss:0.69325\n",
            "Epoch:6/10 Batch:324 Train Loss:0.69293 Val Loss:0.69323\n",
            "Epoch:6/10 Batch:325 Train Loss:0.69323 Val Loss:0.69325\n",
            "Epoch:6/10 Batch:326 Train Loss:0.69316 Val Loss:0.69322\n",
            "Epoch:6/10 Batch:327 Train Loss:0.69296 Val Loss:0.69320\n",
            "Epoch:6/10 Batch:328 Train Loss:0.69303 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:329 Train Loss:0.69307 Val Loss:0.69318\n",
            "Epoch:6/10 Batch:330 Train Loss:0.69292 Val Loss:0.69318\n",
            "Epoch:6/10 Batch:331 Train Loss:0.69306 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:332 Train Loss:0.69258 Val Loss:0.69318\n",
            "Epoch:6/10 Batch:333 Train Loss:0.69347 Val Loss:0.69321\n",
            "Epoch:6/10 Batch:334 Train Loss:0.69290 Val Loss:0.69320\n",
            "Epoch:6/10 Batch:335 Train Loss:0.69331 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:336 Train Loss:0.69331 Val Loss:0.69317\n",
            "Epoch:6/10 Batch:337 Train Loss:0.69255 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:338 Train Loss:0.69314 Val Loss:0.69317\n",
            "Epoch:6/10 Batch:339 Train Loss:0.69335 Val Loss:0.69318\n",
            "Epoch:6/10 Batch:340 Train Loss:0.69289 Val Loss:0.69316\n",
            "Epoch:6/10 Batch:341 Train Loss:0.69293 Val Loss:0.69316\n",
            "Epoch:6/10 Batch:342 Train Loss:0.69330 Val Loss:0.69317\n",
            "Epoch:6/10 Batch:343 Train Loss:0.69311 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:344 Train Loss:0.69328 Val Loss:0.69317\n",
            "Epoch:6/10 Batch:345 Train Loss:0.69289 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:346 Train Loss:0.69295 Val Loss:0.69320\n",
            "Epoch:6/10 Batch:347 Train Loss:0.69303 Val Loss:0.69322\n",
            "Epoch:6/10 Batch:348 Train Loss:0.69254 Val Loss:0.69319\n",
            "Epoch:6/10 Batch:349 Train Loss:0.69322 Val Loss:0.69321\n",
            "Epoch:6/10 Batch:350 Train Loss:0.69199 Val Loss:0.69319\n",
            "Epoch:7/10 Batch:351 Train Loss:0.69263 Val Loss:0.69331\n",
            "Epoch:7/10 Batch:352 Train Loss:0.69301 Val Loss:0.69333\n",
            "Epoch:7/10 Batch:353 Train Loss:0.69138 Val Loss:0.69335\n",
            "Epoch:7/10 Batch:354 Train Loss:0.69477 Val Loss:0.69329\n",
            "Epoch:7/10 Batch:355 Train Loss:0.69294 Val Loss:0.69329\n",
            "Epoch:7/10 Batch:356 Train Loss:0.69386 Val Loss:0.69347\n",
            "Epoch:7/10 Batch:357 Train Loss:0.69430 Val Loss:0.69324\n",
            "Epoch:7/10 Batch:358 Train Loss:0.69388 Val Loss:0.69329\n",
            "Epoch:7/10 Batch:359 Train Loss:0.69455 Val Loss:0.69336\n",
            "Epoch:7/10 Batch:360 Train Loss:0.69205 Val Loss:0.69336\n",
            "Epoch:7/10 Batch:361 Train Loss:0.69218 Val Loss:0.69342\n",
            "Epoch:7/10 Batch:362 Train Loss:0.69369 Val Loss:0.69341\n",
            "Epoch:7/10 Batch:363 Train Loss:0.69429 Val Loss:0.69324\n",
            "Epoch:7/10 Batch:364 Train Loss:0.69596 Val Loss:0.69325\n",
            "Epoch:7/10 Batch:365 Train Loss:0.69250 Val Loss:0.69328\n",
            "Epoch:7/10 Batch:366 Train Loss:0.69483 Val Loss:0.69331\n",
            "Epoch:7/10 Batch:367 Train Loss:0.69357 Val Loss:0.69329\n",
            "Epoch:7/10 Batch:368 Train Loss:0.69249 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:369 Train Loss:0.69384 Val Loss:0.69330\n",
            "Epoch:7/10 Batch:370 Train Loss:0.69299 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:371 Train Loss:0.69156 Val Loss:0.69327\n",
            "Epoch:7/10 Batch:372 Train Loss:0.69354 Val Loss:0.69318\n",
            "Epoch:7/10 Batch:373 Train Loss:0.69296 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:374 Train Loss:0.69395 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:375 Train Loss:0.69190 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:376 Train Loss:0.69284 Val Loss:0.69328\n",
            "Epoch:7/10 Batch:377 Train Loss:0.69255 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:378 Train Loss:0.69351 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:379 Train Loss:0.69379 Val Loss:0.69316\n",
            "Epoch:7/10 Batch:380 Train Loss:0.69278 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:381 Train Loss:0.69302 Val Loss:0.69318\n",
            "Epoch:7/10 Batch:382 Train Loss:0.69311 Val Loss:0.69319\n",
            "Epoch:7/10 Batch:383 Train Loss:0.69310 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:384 Train Loss:0.69311 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:385 Train Loss:0.69291 Val Loss:0.69315\n",
            "Epoch:7/10 Batch:386 Train Loss:0.69335 Val Loss:0.69315\n",
            "Epoch:7/10 Batch:387 Train Loss:0.69297 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:388 Train Loss:0.69308 Val Loss:0.69314\n",
            "Epoch:7/10 Batch:389 Train Loss:0.69285 Val Loss:0.69316\n",
            "Epoch:7/10 Batch:390 Train Loss:0.69325 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:391 Train Loss:0.69317 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:392 Train Loss:0.69329 Val Loss:0.69317\n",
            "Epoch:7/10 Batch:393 Train Loss:0.69306 Val Loss:0.69319\n",
            "Epoch:7/10 Batch:394 Train Loss:0.69277 Val Loss:0.69321\n",
            "Epoch:7/10 Batch:395 Train Loss:0.69325 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:396 Train Loss:0.69270 Val Loss:0.69322\n",
            "Epoch:7/10 Batch:397 Train Loss:0.69337 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:398 Train Loss:0.69205 Val Loss:0.69320\n",
            "Epoch:7/10 Batch:399 Train Loss:0.69332 Val Loss:0.69321\n",
            "Epoch:7/10 Batch:400 Train Loss:0.69202 Val Loss:0.69314\n",
            "Epoch:8/10 Batch:401 Train Loss:0.69430 Val Loss:0.69334\n",
            "Epoch:8/10 Batch:402 Train Loss:0.69469 Val Loss:0.69327\n",
            "Epoch:8/10 Batch:403 Train Loss:0.69151 Val Loss:0.69328\n",
            "Epoch:8/10 Batch:404 Train Loss:0.69064 Val Loss:0.69327\n",
            "Epoch:8/10 Batch:405 Train Loss:0.69283 Val Loss:0.69339\n",
            "Epoch:8/10 Batch:406 Train Loss:0.69448 Val Loss:0.69329\n",
            "Epoch:8/10 Batch:407 Train Loss:0.69214 Val Loss:0.69336\n",
            "Epoch:8/10 Batch:408 Train Loss:0.69399 Val Loss:0.69319\n",
            "Epoch:8/10 Batch:409 Train Loss:0.69080 Val Loss:0.69319\n",
            "Epoch:8/10 Batch:410 Train Loss:0.69361 Val Loss:0.69331\n",
            "Epoch:8/10 Batch:411 Train Loss:0.69044 Val Loss:0.69346\n",
            "Epoch:8/10 Batch:412 Train Loss:0.69266 Val Loss:0.69334\n",
            "Epoch:8/10 Batch:413 Train Loss:0.69386 Val Loss:0.69321\n",
            "Epoch:8/10 Batch:414 Train Loss:0.69002 Val Loss:0.69344\n",
            "Epoch:8/10 Batch:415 Train Loss:0.69389 Val Loss:0.69346\n",
            "Epoch:8/10 Batch:416 Train Loss:0.69518 Val Loss:0.69337\n",
            "Epoch:8/10 Batch:417 Train Loss:0.69725 Val Loss:0.69362\n",
            "Epoch:8/10 Batch:418 Train Loss:0.69302 Val Loss:0.69337\n",
            "Epoch:8/10 Batch:419 Train Loss:0.69232 Val Loss:0.69328\n",
            "Epoch:8/10 Batch:420 Train Loss:0.69090 Val Loss:0.69360\n",
            "Epoch:8/10 Batch:421 Train Loss:0.68787 Val Loss:0.69345\n",
            "Epoch:8/10 Batch:422 Train Loss:0.68985 Val Loss:0.69339\n",
            "Epoch:8/10 Batch:423 Train Loss:0.69178 Val Loss:0.69359\n",
            "Epoch:8/10 Batch:424 Train Loss:0.68689 Val Loss:0.69311\n",
            "Epoch:8/10 Batch:425 Train Loss:0.69732 Val Loss:0.69355\n",
            "Epoch:8/10 Batch:426 Train Loss:0.69148 Val Loss:0.69334\n",
            "Epoch:8/10 Batch:427 Train Loss:0.69451 Val Loss:0.69323\n",
            "Epoch:8/10 Batch:428 Train Loss:0.69448 Val Loss:0.69384\n",
            "Epoch:8/10 Batch:429 Train Loss:0.69296 Val Loss:0.69314\n",
            "Epoch:8/10 Batch:430 Train Loss:0.69260 Val Loss:0.69350\n",
            "Epoch:8/10 Batch:431 Train Loss:0.69146 Val Loss:0.69363\n",
            "Epoch:8/10 Batch:432 Train Loss:0.70113 Val Loss:0.69350\n",
            "Epoch:8/10 Batch:433 Train Loss:0.69136 Val Loss:0.69351\n",
            "Epoch:8/10 Batch:434 Train Loss:0.70108 Val Loss:0.69348\n",
            "Epoch:8/10 Batch:435 Train Loss:0.69778 Val Loss:0.69359\n",
            "Epoch:8/10 Batch:436 Train Loss:0.69301 Val Loss:0.69336\n",
            "Epoch:8/10 Batch:437 Train Loss:0.69719 Val Loss:0.69345\n",
            "Epoch:8/10 Batch:438 Train Loss:0.68896 Val Loss:0.69353\n",
            "Epoch:8/10 Batch:439 Train Loss:0.69045 Val Loss:0.69342\n",
            "Epoch:8/10 Batch:440 Train Loss:0.69314 Val Loss:0.69353\n",
            "Epoch:8/10 Batch:441 Train Loss:0.69553 Val Loss:0.69333\n",
            "Epoch:8/10 Batch:442 Train Loss:0.68822 Val Loss:0.69360\n",
            "Epoch:8/10 Batch:443 Train Loss:0.69313 Val Loss:0.69322\n",
            "Epoch:8/10 Batch:444 Train Loss:0.69311 Val Loss:0.69342\n",
            "Epoch:8/10 Batch:445 Train Loss:0.69432 Val Loss:0.69341\n",
            "Epoch:8/10 Batch:446 Train Loss:0.69441 Val Loss:0.69341\n",
            "Epoch:8/10 Batch:447 Train Loss:0.69180 Val Loss:0.69341\n",
            "Epoch:8/10 Batch:448 Train Loss:0.69527 Val Loss:0.69330\n",
            "Epoch:8/10 Batch:449 Train Loss:0.69281 Val Loss:0.69340\n",
            "Epoch:8/10 Batch:450 Train Loss:0.69301 Val Loss:0.69357\n",
            "Epoch:9/10 Batch:451 Train Loss:0.69092 Val Loss:0.69346\n",
            "Epoch:9/10 Batch:452 Train Loss:0.69483 Val Loss:0.69330\n",
            "Epoch:9/10 Batch:453 Train Loss:0.69525 Val Loss:0.69353\n",
            "Epoch:9/10 Batch:454 Train Loss:0.69625 Val Loss:0.69345\n",
            "Epoch:9/10 Batch:455 Train Loss:0.69028 Val Loss:0.69342\n",
            "Epoch:9/10 Batch:456 Train Loss:0.69299 Val Loss:0.69335\n",
            "Epoch:9/10 Batch:457 Train Loss:0.69008 Val Loss:0.69336\n",
            "Epoch:9/10 Batch:458 Train Loss:0.69500 Val Loss:0.69327\n",
            "Epoch:9/10 Batch:459 Train Loss:0.69191 Val Loss:0.69335\n",
            "Epoch:9/10 Batch:460 Train Loss:0.69266 Val Loss:0.69349\n",
            "Epoch:9/10 Batch:461 Train Loss:0.69472 Val Loss:0.69334\n",
            "Epoch:9/10 Batch:462 Train Loss:0.69179 Val Loss:0.69332\n",
            "Epoch:9/10 Batch:463 Train Loss:0.69512 Val Loss:0.69333\n",
            "Epoch:9/10 Batch:464 Train Loss:0.69454 Val Loss:0.69339\n",
            "Epoch:9/10 Batch:465 Train Loss:0.69294 Val Loss:0.69326\n",
            "Epoch:9/10 Batch:466 Train Loss:0.69450 Val Loss:0.69337\n",
            "Epoch:9/10 Batch:467 Train Loss:0.69355 Val Loss:0.69342\n",
            "Epoch:9/10 Batch:468 Train Loss:0.69244 Val Loss:0.69334\n",
            "Epoch:9/10 Batch:469 Train Loss:0.69156 Val Loss:0.69328\n",
            "Epoch:9/10 Batch:470 Train Loss:0.69218 Val Loss:0.69339\n",
            "Epoch:9/10 Batch:471 Train Loss:0.69101 Val Loss:0.69334\n",
            "Epoch:9/10 Batch:472 Train Loss:0.69486 Val Loss:0.69332\n",
            "Epoch:9/10 Batch:473 Train Loss:0.69170 Val Loss:0.69334\n",
            "Epoch:9/10 Batch:474 Train Loss:0.69283 Val Loss:0.69335\n",
            "Epoch:9/10 Batch:475 Train Loss:0.69570 Val Loss:0.69328\n",
            "Epoch:9/10 Batch:476 Train Loss:0.69262 Val Loss:0.69329\n",
            "Epoch:9/10 Batch:477 Train Loss:0.69361 Val Loss:0.69338\n",
            "Epoch:9/10 Batch:478 Train Loss:0.69105 Val Loss:0.69318\n",
            "Epoch:9/10 Batch:479 Train Loss:0.69143 Val Loss:0.69318\n",
            "Epoch:9/10 Batch:480 Train Loss:0.69250 Val Loss:0.69329\n",
            "Epoch:9/10 Batch:481 Train Loss:0.69396 Val Loss:0.69338\n",
            "Epoch:9/10 Batch:482 Train Loss:0.69392 Val Loss:0.69332\n",
            "Epoch:9/10 Batch:483 Train Loss:0.69439 Val Loss:0.69322\n",
            "Epoch:9/10 Batch:484 Train Loss:0.69037 Val Loss:0.69319\n",
            "Epoch:9/10 Batch:485 Train Loss:0.69384 Val Loss:0.69337\n",
            "Epoch:9/10 Batch:486 Train Loss:0.69369 Val Loss:0.69323\n",
            "Epoch:9/10 Batch:487 Train Loss:0.69164 Val Loss:0.69328\n",
            "Epoch:9/10 Batch:488 Train Loss:0.69373 Val Loss:0.69318\n",
            "Epoch:9/10 Batch:489 Train Loss:0.69165 Val Loss:0.69323\n",
            "Epoch:9/10 Batch:490 Train Loss:0.69335 Val Loss:0.69335\n",
            "Epoch:9/10 Batch:491 Train Loss:0.69232 Val Loss:0.69327\n",
            "Epoch:9/10 Batch:492 Train Loss:0.69158 Val Loss:0.69335\n",
            "Epoch:9/10 Batch:493 Train Loss:0.69088 Val Loss:0.69324\n",
            "Epoch:9/10 Batch:494 Train Loss:0.69442 Val Loss:0.69330\n",
            "Epoch:9/10 Batch:495 Train Loss:0.69303 Val Loss:0.69320\n",
            "Epoch:9/10 Batch:496 Train Loss:0.69342 Val Loss:0.69331\n",
            "Epoch:9/10 Batch:497 Train Loss:0.69227 Val Loss:0.69338\n",
            "Epoch:9/10 Batch:498 Train Loss:0.69292 Val Loss:0.69346\n",
            "Epoch:9/10 Batch:499 Train Loss:0.69376 Val Loss:0.69339\n",
            "Epoch:9/10 Batch:500 Train Loss:0.69297 Val Loss:0.69339\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
